{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subject_filter(x, subjects):\n",
    "    if x['subject'] in subjects:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "dataset = load_dataset(\"cais/mmlu\", \"all\")\n",
    "subjects = ['machine_learning', 'econometrics', 'abstract_algebra', 'professional_accounting', 'professional_medicine']\n",
    "\n",
    "filtered_dataset = dataset.filter(lambda x: subject_filter(x, subjects)).filter(lambda example: len(example[\"question\"]) < 128)\n",
    "\n",
    "sampled_dataset = filtered_dataset.shuffle(seed=42)['test'][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = np.array(sampled_dataset[\"question\"])\n",
    "choices = np.array(sampled_dataset[\"choices\"])\n",
    "answers = np.array(sampled_dataset[\"answer\"])\n",
    "\n",
    "data = {\"Original\": questions, \"Choice1\": choices[:, 0], \"Choice2\": choices[:, 1], \"Choice3\": choices[:, 2], \"Choice4\": choices[:, 3], 'Answer': answers}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df[['Choice1', 'Choice2', 'Choice3', 'Choice4', 'Answer', 'Original']].to_csv(\"mmlu_hard.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from datasets import load_dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "client = OpenAI(api_key=\"...\")\n",
    "\n",
    "content = \"\"\"Given a sentences and a set of possible answers, transform the sentence into a plain question.\n",
    "\n",
    "Here are some examples:\n",
    "SENTENCE: Some birds are\n",
    "CHOICES: (A) people (B) creatures (C) fish (D) solar\n",
    "CONVERTED: What are some birds?\n",
    "\n",
    "SENTENCE: A reptile's body temperature\n",
    "CHOICES: (A) will sync with their climate (B) will keep stable under any circumstances (C) reacts as other warm blooded animals temperature would (D) plunges rapidly in warm climates\n",
    "CONVERTED: What does a reptile's body temperature do?\n",
    "\n",
    "SENTENCE: If this fell on you, you would probably die\n",
    "CHOICES: (A) a leaning tower (B) a balloon (C) a feather (D) a towel\n",
    "CONVERTED: Which one, if fallen on you, would probably kill you?\n",
    "\n",
    "You don't have to answer, just convert the SENTENCE.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def convert_mmlu(row, model=\"gpt-4-turbo-preview\"):\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": content\n",
    "        },\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"SENTENCE: {row['question']}\\nCHOICES: (A) {row['choice1']} (B) {row['choice2']} (C) {row['choice3']} (D) {row['choice4']}\\nCONVERTED:\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.3,\n",
    "    max_tokens=64,\n",
    "    top_p=0.5,\n",
    "    seed=42\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "df['Converted'] = df.progress_apply(convert_mmlu, axis=1)\n",
    "df['Interrogative'] = df['Converted'].apply(lambda x: x.split(\"CONVERTED: \")[-1]).apply(lambda x: x if x[-1] == '?' else x + '?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaratives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "int2dec_sys = \"\"\"You are a very diligent AI assistant. Do the following conversions:\n",
    "\n",
    "User: What do national parks have rules for?\n",
    "Assistant: National parks have rules for...\n",
    "\n",
    "User: What is a disperser able to do?\n",
    "Assistant: A disperser is able to perform...\n",
    "\n",
    "User: When do nocturnal predators hunt?\n",
    "Assistant: Nocturnal predators hunt during...\"\"\"\n",
    "\n",
    "def int2dec(prompt, model=\"gpt-4-turbo-preview\"):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": int2dec_sys\n",
    "            },\n",
    "            {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"User: {prompt}\"\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.3,\n",
    "        max_tokens=128,\n",
    "        top_p=0.5,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imperatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int2imp_sys = \"\"\"You are a very diligent AI assistant. Convert questions into instructions:\n",
    "\n",
    "User: What do national parks have rules for?\n",
    "Assistant: Tell me which of followings are rules of national parks.\n",
    "\n",
    "User: What is a disperser able to do?\n",
    "Assistant: Explain what a disperser can do.\n",
    "\n",
    "User: When do nocturnal predators hunt?\n",
    "Assistant: Tell me when nocturnal predators hunt.\"\"\"\n",
    "\n",
    "def int2imp(prompt, model=\"gpt-4-turbo-preview\"):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": int2imp_sys\n",
    "            },\n",
    "            {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"User: {prompt}\"\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.3,\n",
    "        max_tokens=128,\n",
    "        top_p=0.5,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "# %%\n",
    "df['Declarative'] = df['Interrogative'].progress_apply(int2dec)\n",
    "df['Imperative'] = df['Interrogative'].progress_apply(int2imp)\n",
    "# %%\n",
    "df.rename(columns={\"question\": \"Original\"})\n",
    "df.drop('Converted', axis=1).to_csv(\"mmlu_conv.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
